{
  
    
        "post0": {
            "title": "빅데이터 개론 정리(중간 이후)",
            "content": "import pandas as pd import numpy as np import scipy.stats as stats from itertools import product import matplotlib.pyplot as plt %matplotlib inline . &#54632;&#49688; . np . np.sqrt(2) . 1.4142135623730951 . np.log(10) . 2.302585092994046 . np.sin(3) . 0.1411200080598672 . np.arange(10) . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . np.arange(10,20,0.5) . array([10. , 10.5, 11. , 11.5, 12. , 12.5, 13. , 13.5, 14. , 14.5, 15. , 15.5, 16. , 16.5, 17. , 17.5, 18. , 18.5, 19. , 19.5]) . a = np.arange(10) . max(a), np.max(a) . (9, 9) . sum(a), np.sum(a) . (45, 45) . np.mean(a) . 4.5 . np.cumsum(a) # x 의 원소들의 누적합(cumulative sum) . array([ 0, 1, 3, 6, 10, 15, 21, 28, 36, 45]) . &#54632;&#49688; &#47564;&#46308;&#44592; . &#49707;&#51088; &#48289;&#53552; &#46608;&#45716; &#47532;&#49828;&#53944;&#50640; &#51080;&#45716; &#50577;&#49688;&#46308;&#51032; &#54633; . my_numbers = np.arange(-10,10) my_sum = 0 for value in my_numbers : if value &gt; 0 : my_sum = my_sum + value my_sum . 45 . &#54632;&#49688; &#50696;&#51228; -&#47560;&#51060;&#45320;&#49828;&#47484; &#54540;&#47084;&#49828;&#47196; &#48148;&#44984;&#45716; &#54632;&#49688;(&#51208;&#45824;&#44050;) . def minus2plus(x) : if x &lt; 0: res = -x else : res = x return res . minus2plus(-2) . 2 . minus2plus(4.0) . 4.0 . minus2plus(0.0) . 0.0 . &#50668;&#47084; &#44060;&#51032; &#44050;&#51012; &#44032;&#51652; &#47532;&#49828;&#53944;&#45208; &#48289;&#53552;&#47484; &#51064;&#51088;&#47196; &#54616;&#45716; &#54632;&#49688; &#47564;&#46308;&#44592; . &#50577;&#49688;&#46308;&#47564;&#51032; &#54633;&#51012; &#44396;&#54616;&#45716; &#54632;&#49688; sum_positive . def sum_positive(x) : sum_plus = 0 for value in x : if value &gt; 0 : sum_plus = sum_plus + value return sum_plus #양수들의 합을 구해주는 함수 . # my_sum = 0 # for value in my_numbers : # if value &gt; 0 : # my_sum = my_sum + value # my_sum . sum_positive(my_numbers) . 45 . your_numbers = np.arange(100) . your_sum = sum_positive(your_numbers) # 함수의 결과를 새로운 변수에 저장 your_sum . 4950 . &#47928;&#51088;&#50676;&#51060; &#54252;&#54632;&#46108; &#50896;&#49548;&#44032; &#47751; &#44060;&#44032; &#51080;&#45716;&#51648; &#49464;&#50612;&#51452;&#45716; &#54632;&#49688; count_char . def count_char(x, c) : count = 0 for value in x : if c in value : count = count + 1 return count . student_names = [&#39;김철수&#39;, &#39;이용희&#39;, &#39;손흥민&#39;, &#39;방탄소년&#39;, &#39;김용희&#39;, &#39;박서준&#39;, &#39;김지원&#39;] count_char(student_names, &quot;용희&quot;) . 2 . yoot_draw = [&#39;등&#39;, &#39;등&#39;, &#39;배&#39;, &#39;등&#39;] count_char(yoot_draw, &quot;등&quot;) . 3 . &#47700;&#49548;&#46300; . x = [1,2,3,4] x.append(5) x . [1, 2, 3, 4, 5] . x.append(100) x . [1, 2, 3, 4, 5, 100] . df = pd.DataFrame({&#39;x&#39;: [-10,2,5,-4], &#39;y&#39;: [1, 2, -3, 2]}) df . x y . 0 -10 | 1 | . 1 2 | 2 | . 2 5 | -3 | . 3 -4 | 2 | . df.sum() . x -7 y 2 dtype: int64 . sum(df.x) . -7 . &#54665;&#51032; &#54633; &#44396;&#54616;&#44592; . df.sum(axis=1) . 0 -9 1 4 2 2 3 -2 dtype: int64 . &#45936;&#51060;&#53552; &#54532;&#47112;&#51076;&#50640; &#54632;&#49688; &#51201;&#50857;&#54616;&#44592; . dataframe.applymap(function) . df.applymap(minus2plus) . x y . 0 10 | 1 | . 1 2 | 2 | . 2 5 | 3 | . 3 4 | 2 | . df.apply(sum_positive, axis=0) # axis = 0 은 함수를 각 열에 대하여 적용 . x 7 y 5 dtype: int64 . df.apply(sum_positive, axis=1) # axis = 1 : 각 행에 대하여 적용 . 0 1 1 4 2 5 3 2 dtype: int64 . 2&#44060; &#51060;&#49345;&#51032; &#51064;&#51088;&#47484; &#54632;&#49688;&#50640; &#51201;&#50857;&#54616;&#44592; . # .... # .... # return ... # dataframe.apply(function, x=2, z=&quot;a&quot;, axis=1) . df2 = pd.DataFrame({ &#39;name&#39;: student_names}) df2 . name . 0 김철수 | . 1 이용희 | . 2 손흥민 | . 3 방탄소년 | . 4 김용희 | . 5 박서준 | . 6 김지원 | . df2.apply(count_char, c=&quot;용희&quot;, axis=0) #원하는 문자열을 세어주는 함수에다가 용희를 검색하게 함, 열 단위로 . name 2 dtype: int64 . df2.apply(count_char, c=&quot;김&quot;, axis=0) #김씨인 사람 . name 3 dtype: int64 . &#54632;&#49688;&#51032; &#45936;&#51060;&#53552; &#52712;&#44553; . another_function = minus2plus # 다른 이름을 가진 함수로 정의하기 . another_function(-20) #똑같이 적용됨을 알 수 있음 . 20 . 보너스를 주는 규칙을 함수로 만들었다고 하자. 연봉이 3000만원 이하면 연봉의 20%, 3000만원 초과면 15%를 보너스로 준다. . def bonus_amount(salary): if salary &lt;= 3000 : bonus = salary * 0.2 else : bonus = salary * 0.15 return bonus . bonus_amount(2000) . 400.0 . bonus_amount(4000) . 600.0 . 직원 1명을 고용해서 드는 총비용이 연봉(salary), 보험료(insurance), 야근 수당(night), 보너스를 합친 액수라고 하자. 이러한 총비용을 계산하는 함수 total_expense 를 다음과 같이 정의하자. 함수 total_expense 의 마지막 인자는 보너스를 계산하는 함수 이름이다. . def total_expense(salary, insurance, night, bonus_function) : expense = salary + insurance + night + bonus_function(salary) return expense . total_expense(4000, 200, 100, bonus_amount) #bonus_amount는 함수임에도 불구하고 인자처럼 쓰일 수 있음 . 4900.0 . 모든 직원에게 1000만원 특별 보너스를 지급하려고 한다. 새로운 함수 bonus_amount_special 를 만들고 총비융을 계산하는 함수에 인자로 보내주면 된다. . def bonus_amount_special(salary): if salary &lt;= 3000 : bonus = salary * 0.2 else : bonus = salary * 0.15 bonus = bonus + 1000 return bonus . total_expense(4000, 200, 100, bonus_amount_special) . 5900.0 . &#54632;&#49688; &#47928;&#51228; - &#51452;&#49324;&#50948; &#44172;&#51076; &#54632;&#49688;&#54868; . 나와 규빈에게 각가 200000원이 있다. 주사위를 던져 2 이하의 눈이 나오면 내가 승리하고 10000원을 규빈으로부터 받는다. 반면 3 이상이 나오면 규빈이 승리하고 규빈에게 8000원을 주어야 한다. . gu = 200000 me = 200000 for i in np.arange(20): x = np.random.rand(1) if x &lt; 1/3: me += 10000 gu -= 10000 else : me -= 6000 gu += 6000 print([me, gu]) . [194000, 206000] [204000, 196000] [214000, 186000] [208000, 192000] [202000, 198000] [212000, 188000] [206000, 194000] [216000, 184000] [226000, 174000] [220000, 180000] [214000, 186000] [208000, 192000] [218000, 182000] [212000, 188000] [222000, 178000] [216000, 184000] [226000, 174000] [220000, 180000] [214000, 186000] [224000, 176000] . def game_m(me_i, gu_i, me_r, gu_r, rep, prob): me = me_i gu = gu_i for i in np.arange(rep): x = np.random.rand(1) if x &lt; prob : me += me_r gu -= me_r else : me -= gu_r gu += gu_r print([me, gu]) final = [me, gu] #print(final) plt.bar(np.arange(2),final) #두개 return final . game_m(200000, 200000, 1000, 6000, 20, 1/3) . [194000, 206000] [195000, 205000] [196000, 204000] [197000, 203000] [198000, 202000] [199000, 201000] [200000, 200000] [194000, 206000] [195000, 205000] [196000, 204000] [190000, 210000] [184000, 216000] [178000, 222000] [179000, 221000] [180000, 220000] [174000, 226000] [175000, 225000] [169000, 231000] [163000, 237000] [164000, 236000] . [164000, 236000] . &#54364;&#48376; . 표본(sample)은 분석의 대상이 되는 전체집단, 즉 모집단(population)의 일부분이다. . 철수가 선거구의 가장 번화한 거리에 나가서 지나가는 사람들 1000명을 면담하고 후보의 지지율이 55% 라는 정보를 얻었다고 하자. 조사한 55%의 지지율이 실제 전체 유권자의 지지율과 얼마나 차이가 나는지 알 수 없다면 이러한 정보 수집은 의미가 있는가? . 가장 번화한 거리를 지나가는 사람들은 매우 다양하기 때문에 전체 유권자를 잘 대표할 수 있고 또한 1000명은 상당한 큰 수이므로 자신의 표본은 쓸만하다고 철수가 주장한다면 여러분은 여기에 동의하는가? . 이제 영이가 철수의 방법보다 더 나은 방법이 있다고 주장한다. 영이가 제안한 1000명의 표본을 추출하는 방법은 다음과 같다. . 임의로 추출되었다(randomly selected)는 의미를 쉽게 표현하면 전화번호가 수록된 명부에서 유권자 1000명을 눈감고 뽑았다는 의미이다. 물론 같은 사람을 두 번 이상 뽑지는 않는다. . 눈감고 뽑았다는 의미는 모든 유권자가 표본에 포함될 가능성이 같다 는 것이다. 이렇게 “눈 감고 표본을 추출” 하는 방법을 단순임의추출(simple random sampling) 이라고 하며 추출된 표본을 확률표본(random sample, probability sample) 이라고 부른다. . 철수의 방법으로 얻은 표본을 비확률표본 또는 편의표본((convenience sample)이라고 부른다. 데이터과학에서 다루는 표본은 거의 대부분 확률표본이다. . 확률표본: 모집단의 속한 모든 개체가 표본에 속할 가능성이 같은 경우(철수) . | 편의표본: 개체가 표본에 포함되는 가능성을 알 수 없는 표본(영이) . | . url1 = &quot;https://ilovedata.github.io/teaching/bigdata2/data/seoul_bike_201909_3.csv&quot; bike = pd.read_csv(url1, encoding=&quot;CP949&quot;) . bike.head(10) . 자전거번호 대여일시 대여 대여소번호 대여 대여소명 대여거치대 반납일시 반납대여소번호 반납대여소명 반납거치대 이용시간 이용거리 . 0 SPB-17003 | 2019-09-28 16:10:55 | 368 | SK 서린빌딩 앞 | 4 | 2019-09-28 17:03:32 | 2002 | 노들역 1번출구 | 14 | 52 | 8940.0 | . 1 SPB-14405 | 2019-09-28 16:48:16 | 2024 | 상도역 1번출구 | 3 | 2019-09-28 17:03:44 | 2002 | 노들역 1번출구 | 18 | 15 | 1910.0 | . 2 SPB-18431 | 2019-09-28 16:59:54 | 2002 | 노들역 1번출구 | 10 | 2019-09-28 17:03:57 | 2002 | 노들역 1번출구 | 10 | 2 | 30.0 | . 3 SPB-04853 | 2019-09-28 15:31:49 | 207 | 여의나루역 1번출구 앞 | 32 | 2019-09-28 17:10:12 | 2002 | 노들역 1번출구 | 19 | 98 | 9610.0 | . 4 SPB-11122 | 2019-09-28 15:35:41 | 207 | 여의나루역 1번출구 앞 | 14 | 2019-09-28 17:10:37 | 2002 | 노들역 1번출구 | 18 | 90 | 9450.0 | . 5 SPB-23089 | 2019-09-28 17:02:37 | 2003 | 사육신공원앞 | 5 | 2019-09-28 17:13:44 | 2002 | 노들역 1번출구 | 10 | 10 | 1410.0 | . 6 SPB-15669 | 2019-09-28 16:17:54 | 2213 | 고속터미널역 5번출구 앞 | 7 | 2019-09-28 17:14:22 | 2002 | 노들역 1번출구 | 8 | 56 | 0.0 | . 7 SPB-09727 | 2019-09-28 17:04:56 | 2002 | 노들역 1번출구 | 5 | 2019-09-28 17:17:10 | 2002 | 노들역 1번출구 | 17 | 11 | 380.0 | . 8 SPB-10053 | 2019-09-28 16:59:45 | 2002 | 노들역 1번출구 | 3 | 2019-09-28 17:17:53 | 2002 | 노들역 1번출구 | 5 | 17 | 1080.0 | . 9 SPB-14487 | 2019-09-28 16:01:41 | 2024 | 상도역 1번출구 | 5 | 2019-09-28 17:20:44 | 2002 | 노들역 1번출구 | 14 | 78 | 13870.0 | . bike.shape . (407589, 11) . bike.T . 0 1 2 3 4 5 6 7 8 9 ... 407579 407580 407581 407582 407583 407584 407585 407586 407587 407588 . 자전거번호 SPB-17003 | SPB-14405 | SPB-18431 | SPB-04853 | SPB-11122 | SPB-23089 | SPB-15669 | SPB-09727 | SPB-10053 | SPB-14487 | ... | SPB-04848 | SPB-03378 | SPB-12697 | SPB-16477 | SPB-18051 | SPB-24072 | SPB-16130 | SPB-03728 | SPB-08928 | SPB-06988 | . 대여일시 2019-09-28 16:10:55 | 2019-09-28 16:48:16 | 2019-09-28 16:59:54 | 2019-09-28 15:31:49 | 2019-09-28 15:35:41 | 2019-09-28 17:02:37 | 2019-09-28 16:17:54 | 2019-09-28 17:04:56 | 2019-09-28 16:59:45 | 2019-09-28 16:01:41 | ... | 2019-09-03 08:54:59 | 2019-09-03 09:37:57 | 2019-09-05 08:53:09 | 2019-09-09 08:53:39 | 2019-09-10 08:58:57 | 2019-09-12 08:56:34 | 2019-09-18 10:13:09 | 2019-09-25 08:00:28 | 2019-09-30 07:49:27 | 2019-09-30 09:58:43 | . 대여 대여소번호 368 | 2024 | 2002 | 207 | 207 | 2003 | 2213 | 2002 | 2002 | 2024 | ... | 230 | 99999 | 240 | 240 | 240 | 240 | 99999 | 2183 | 2183 | 99999 | . 대여 대여소명 SK 서린빌딩 앞 | 상도역 1번출구 | 노들역 1번출구 | 여의나루역 1번출구 앞 | 여의나루역 1번출구 앞 | 사육신공원앞 | 고속터미널역 5번출구 앞 | 노들역 1번출구 | 노들역 1번출구 | 상도역 1번출구 | ... | 영등포구청역 1번출구 | 영남단말기정비 | 문래역 4번출구 앞 | 문래역 4번출구 앞 | 문래역 4번출구 앞 | 문래역 4번출구 앞 | 영남단말기정비 | 동방1교 | 동방1교 | 영남단말기정비 | . 대여거치대 4 | 3 | 10 | 32 | 14 | 5 | 7 | 5 | 3 | 5 | ... | 10 | 1 | 5 | 1 | 5 | 9 | 1 | 7 | 10 | 5 | . 반납일시 2019-09-28 17:03:32 | 2019-09-28 17:03:44 | 2019-09-28 17:03:57 | 2019-09-28 17:10:12 | 2019-09-28 17:10:37 | 2019-09-28 17:13:44 | 2019-09-28 17:14:22 | 2019-09-28 17:17:10 | 2019-09-28 17:17:53 | 2019-09-28 17:20:44 | ... | 2019-09-03 09:03:55 | 2019-09-03 10:35:28 | 2019-09-05 08:59:21 | 2019-09-09 09:27:21 | 2019-09-10 09:10:40 | 2019-09-12 09:03:37 | 2019-09-18 11:38:30 | 2019-09-25 08:54:02 | 2019-09-30 09:42:27 | 2019-09-30 13:01:26 | . 반납대여소번호 2002 | 2002 | 2002 | 2002 | 2002 | 2002 | 2002 | 2002 | 2002 | 2002 | ... | 99999 | 99999 | 99999 | 99999 | 99999 | 99999 | 99999 | 99999 | 99999 | 99999 | . 반납대여소명 노들역 1번출구 | 노들역 1번출구 | 노들역 1번출구 | 노들역 1번출구 | 노들역 1번출구 | 노들역 1번출구 | 노들역 1번출구 | 노들역 1번출구 | 노들역 1번출구 | 노들역 1번출구 | ... | 영남단말기정비 | 영남단말기정비 | 영남단말기정비 | 영남단말기정비 | 영남단말기정비 | 영남단말기정비 | 영남단말기정비 | 영남단말기정비 | 영남단말기정비 | 영남단말기정비 | . 반납거치대 14 | 18 | 10 | 19 | 18 | 10 | 8 | 17 | 5 | 14 | ... | 2 | 1 | 2 | 2 | 2 | 2 | 1 | 5 | 7 | 5 | . 이용시간 52 | 15 | 2 | 98 | 90 | 10 | 56 | 11 | 17 | 78 | ... | 8 | 57 | 5 | 33 | 11 | 6 | 85 | 53 | 2 | 182 | . 이용거리 8940.0 | 1910.0 | 30.0 | 9610.0 | 9450.0 | 1410.0 | 0.0 | 380.0 | 1080.0 | 13870.0 | ... | 1370.0 | 10.0 | 910.0 | 1100.0 | 960.0 | 720.0 | 40.0 | 12910.0 | 0.0 | 10.0 | . 11 rows × 407589 columns . bike.columns . Index([&#39;자전거번호&#39;, &#39;대여일시&#39;, &#39;대여 대여소번호&#39;, &#39;대여 대여소명&#39;, &#39;대여거치대&#39;, &#39;반납일시&#39;, &#39;반납대여소번호&#39;, &#39;반납대여소명&#39;, &#39;반납거치대&#39;, &#39;이용시간&#39;, &#39;이용거리&#39;], dtype=&#39;object&#39;) . . bike.plot.hist(y=&quot;이용거리&quot;, bins= 30); . bike.plot.box(y=&quot;이용거리&quot;); . /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51060 ( N{HANGUL SYLLABLE I}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50857 ( N{HANGUL SYLLABLE YONG}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44144 ( N{HANGUL SYLLABLE GEO}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47532 ( N{HANGUL SYLLABLE RI}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) . bike[[&quot;이용거리&quot;]].describe() . 이용거리 . count 407589.000000 | . mean 4253.336228 | . std 5782.673901 | . min 0.000000 | . 25% 1200.000000 | . 50% 2380.000000 | . 75% 5130.000000 | . max 153490.000000 | . &#49368;&#54540; &#52628;&#52636; . 데이터프레임에서 행, 즉 레코드를 단순임의추출 방법으로 추출하는 메소드는 sample()이다. . DataFrame.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False) . n: 표본의 크기 . | frac: 표본의 비율 . | replace : 복원추출에 대한 여부 . | random_state : 표본추출시 사용되는 난수를 지정. 지정한 난수가 같으면 동일한 표본이 추출된다. . | . bike_sample_100 = bike.sample(n=100, replace=False, random_state=1) bike_sample_100.head(10) . 자전거번호 대여일시 대여 대여소번호 대여 대여소명 대여거치대 반납일시 반납대여소번호 반납대여소명 반납거치대 이용시간 이용거리 . 375631 SPB-11285 | 2019-09-21 10:06:07 | 1452 | 겸재교 진입부 | 9 | 2019-09-21 10:38:10 | 3520 | 광진경찰서 | 4 | 30 | 5920.0 | . 219156 SPB-10989 | 2019-09-24 22:36:38 | 2321 | 학여울역 사거리 | 9 | 2019-09-24 22:41:46 | 2320 | 도곡역 대치지구대 방향 | 1 | 4 | 0.0 | . 2646 SPB-20213 | 2019-09-11 15:51:03 | 1986 | 태평양물산빌딩 | 8 | 2019-09-11 16:28:15 | 2007 | 유한양행앞 | 6 | 37 | 5450.0 | . 329716 SPB-05859 | 2019-09-11 09:09:47 | 1912 | 한신휴아파트 앞 | 9 | 2019-09-11 09:15:23 | 2801 | 대륭포스트타워1차 | 14 | 5 | 830.0 | . 35470 SPB-03699 | 2019-09-01 14:19:17 | 2265 | 이수고가차도 남단 | 2 | 2019-09-01 14:34:28 | 2060 | 남성역3번출구 뒤 | 6 | 14 | 2160.0 | . 132032 SPB-03922 | 2019-09-20 22:09:34 | 274 | 영등포역지하쇼핑센타 5번출구 | 1 | 2019-09-20 23:14:57 | 2219 | 고속터미널역 8-1번, 8-2번 출구 사이 | 9 | 65 | 11820.0 | . 15067 SPB-17943 | 2019-09-17 18:24:58 | 2002 | 노들역 1번출구 | 18 | 2019-09-17 19:57:08 | 2025 | 흑석역 1번출구 | 3 | 91 | 8560.0 | . 226253 SPB-07462 | 2019-09-03 18:27:38 | 2358 | 구룡초사거리 (현대아파트10동 앞 ) | 4 | 2019-09-03 18:41:48 | 2329 | 르네상스호텔사거리 역삼지하보도 2번출구 | 6 | 13 | 2520.0 | . 318420 SPB-13058 | 2019-09-10 15:52:12 | 2620 | 송파나루역 4번 출구옆 | 7 | 2019-09-10 16:05:27 | 2622 | 올림픽공원역 3번출구 | 8 | 13 | 2220.0 | . 405911 SPB-18550 | 2019-09-09 19:10:55 | 549 | 아차산역 3번출구 | 5 | 2019-09-09 19:19:11 | 3542 | 래미안 구의파크 스위트 | 17 | 7 | 1090.0 | . bike_sample_100.plot.hist(y=&quot;이용거리&quot;, bins= 30); . bike_sample_100.plot.box(y=&quot;이용거리&quot;); . /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51060 ( N{HANGUL SYLLABLE I}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50857 ( N{HANGUL SYLLABLE YONG}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44144 ( N{HANGUL SYLLABLE GEO}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47532 ( N{HANGUL SYLLABLE RI}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) . bike_sample_100[[&quot;이용거리&quot;]].describe() . 이용거리 . count 100.000000 | . mean 3824.100000 | . std 5524.629901 | . min 0.000000 | . 25% 1157.500000 | . 50% 2170.000000 | . 75% 4285.000000 | . max 45700.000000 | . 추출된 표본에 속한 데이터의 분포는 경험적 분포(empirical distribution)라고 부른다. 경험적이라는 의미는 추출된 관측값들의 분포 라는 것이다. 대부분 경우 모집단 전체를 관측할 수 없기 때문에 모집단에 대해서는 경험적 분포라고 부르지 않고 모집단 분포(population distribution)라고 따로 부른다. . 확률표본의 경험적 분포는 표본의 수가 증가하면 모집단의 분포와 점점 유사하게 나타날 것이라고 예상할 수 있다. 즉, 표본의 개수(sample size)가 증가하면 확률표본에서 얻은 정보는 모집단에 대한 정보와 점점 가까워 진다. . 위에서 영이의 표본은 확률표본이므로 추출된 유권자의 수가 증가하면 표본으로 부터 얻은 지지율이 모집단의 지지율에 가까워진다. 표본의 크기가 커지면 확률표본의 경험적 분포가 모집단의 분포에 점점 가까워 지는 법칙을 대수의 법칙 (law of large number)라고 한다. . . bike_sample_10000 = bike.sample(n=10000, replace=False, random_state=3) . bike_sample_10000.plot.hist(y=&quot;이용거리&quot;, bins= 30); . bike_sample_10000.plot.box(y=&quot;이용거리&quot;); . /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51060 ( N{HANGUL SYLLABLE I}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50857 ( N{HANGUL SYLLABLE YONG}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44144 ( N{HANGUL SYLLABLE GEO}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) /Users/heji/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47532 ( N{HANGUL SYLLABLE RI}) missing from current font. fig.canvas.print_figure(bytes_io, **kw) . fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(15,10)) axes[0].boxplot(bike_sample_100.이용거리) axes[0].set_ylim(0,153500) axes[0].set_title(&#39;n=100&#39;) axes[1].boxplot(bike_sample_10000.이용거리) axes[1].set_ylim(0,153500) axes[1].set_title(&#39;n=10000&#39;) axes[2].boxplot(bike.이용거리) axes[2].set_ylim(0,153500) axes[2].set_title(&#39;Population&#39;) plt.show() . 표본의 크기가 작을 때는 표본 평균이 모집단의 평균에서 멀어진 경우가 많이 나타나지만 표본의 크기가 증가하면서 모집단의 평균에 점점 가까워지는 경향을 보인다. | . n = np.arange(10,2021,20) n . array([ 10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290, 310, 330, 350, 370, 390, 410, 430, 450, 470, 490, 510, 530, 550, 570, 590, 610, 630, 650, 670, 690, 710, 730, 750, 770, 790, 810, 830, 850, 870, 890, 910, 930, 950, 970, 990, 1010, 1030, 1050, 1070, 1090, 1110, 1130, 1150, 1170, 1190, 1210, 1230, 1250, 1270, 1290, 1310, 1330, 1350, 1370, 1390, 1410, 1430, 1450, 1470, 1490, 1510, 1530, 1550, 1570, 1590, 1610, 1630, 1650, 1670, 1690, 1710, 1730, 1750, 1770, 1790, 1810, 1830, 1850, 1870, 1890, 1910, 1930, 1950, 1970, 1990, 2010]) . len(n) . 101 .",
            "url": "https://g-gg-ggg.github.io/Oops/python/2022/11/01/.html",
            "relUrl": "/python/2022/11/01/.html",
            "date": " • Nov 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "빅데이터 개론 4, 5 복습",
            "content": "import pandas as pd . df = pd.DataFrame( { &#39;name&#39; : [&#39;이철수&#39;, &#39;김영희&#39;, &#39;홍길동&#39;, &#39;John Smith&#39;, &#39;Mary Doe&#39;], &#39;sex&#39; : [&#39;M&#39;, &#39;F&#39;, &#39;M&#39;, &#39;M&#39;, &#39;F&#39;], &#39;age&#39; : [ 23, 25, 21, 33, 45], &#39;height&#39; : [153.5, 175.3, 163.4, 180.0, 165.7] } ) . df . name sex age height . 0 이철수 | M | 23 | 153.5 | . 1 김영희 | F | 25 | 175.3 | . 2 홍길동 | M | 21 | 163.4 | . 3 John Smith | M | 33 | 180.0 | . 4 Mary Doe | F | 45 | 165.7 | . df[&#39;name&#39;][0] . &#39;이철수&#39; . df[[&#39;name&#39;, &#39;age&#39;]] . name age . 0 이철수 | 23 | . 1 김영희 | 25 | . 2 홍길동 | 21 | . 3 John Smith | 33 | . 4 Mary Doe | 45 | . df.sex . 0 M 1 F 2 M 3 M 4 F Name: sex, dtype: object . df[df[&#39;sex&#39;] == &#39;M&#39;] . name sex age height . 0 이철수 | M | 23 | 153.5 | . 2 홍길동 | M | 21 | 163.4 | . 3 John Smith | M | 33 | 180.0 | . .isin &#54632;&#49688;&#51032; &#44292;&#54840; &#50504;&#50640; &#49440;&#53469;&#54624; &#47928;&#51088; &#46608;&#45716; &#49707;&#51088;&#47484; &#47532;&#49828;&#53944; &#54805;&#53468;&#47196; &#45347;&#50612;&#51456;&#45796;. . df[df[&#39;age&#39;].isin([25,33])] . name sex age height . 1 김영희 | F | 25 | 175.3 | . 3 John Smith | M | 33 | 180.0 | . df[ (df[&#39;age&#39;] &lt;= 30) | (df[&#39;height&#39;] &lt; 160.0) ] # | : 또는 . name sex age height . 0 이철수 | M | 23 | 153.5 | . 1 김영희 | F | 25 | 175.3 | . 2 홍길동 | M | 21 | 163.4 | . df_1 = df.loc[ (df[&#39;height&#39;] &gt;= 170.0) , &#39;name&#39;] df_1 . 1 김영희 3 John Smith Name: name, dtype: object . df . name sex age height . 0 이철수 | M | 23 | 153.5 | . 1 김영희 | F | 25 | 175.3 | . 2 홍길동 | M | 21 | 163.4 | . 3 John Smith | M | 33 | 180.0 | . 4 Mary Doe | F | 45 | 165.7 | . import pandas as pd . house = pd.read_csv(&quot;https://ilovedata.github.io/teaching/bigdata2/data/korea_house_data_01.csv&quot;, encoding=&quot;CP949&quot;) . house.head(10) . 시점 행정구역별(읍면동) 일반가구_계 1인 2인 3인 4인 5인 6인 7인 이상 . 0 2015 | 전국 | 19111030 | 5203440 | 4993818 | 4100979 | 3588931 | 940413 | 217474 | 65975 | . 1 2015 | 서울특별시 | 3784490 | 1115744 | 930467 | 817440 | 701945 | 169436 | 38547 | 10911 | . 2 2015 | 부산광역시 | 1335900 | 361749 | 366048 | 296742 | 238031 | 56971 | 12928 | 3431 | . 3 2015 | 대구광역시 | 928528 | 239517 | 239824 | 208795 | 184258 | 44071 | 9582 | 2481 | . 4 2015 | 인천광역시 | 1045417 | 243678 | 265079 | 245135 | 220538 | 55230 | 12253 | 3504 | . 5 2015 | 광주광역시 | 567157 | 163577 | 137662 | 115701 | 109612 | 32199 | 6647 | 1759 | . 6 2015 | 대전광역시 | 582504 | 169391 | 140603 | 122088 | 112055 | 30022 | 6489 | 1856 | . 7 2015 | 울산광역시 | 423412 | 103551 | 104101 | 100230 | 90735 | 19914 | 3851 | 1030 | . 8 2015 | 세종특별자치시 | 75219 | 21889 | 18010 | 15219 | 14560 | 4238 | 994 | 309 | . 9 2015 | 경기도 | 4384742 | 1026471 | 1062222 | 1005330 | 971146 | 246638 | 56117 | 16818 | . . house.sort_values(by = [ &#39;행정구역별(읍면동)&#39;, &#39;시점&#39;]) . 시점 행정구역별(읍면동) 일반가구_계 1인 2인 3인 4인 5인 6인 7인 이상 . 10 2015 | 강원도 | 606117 | 189379 | 178470 | 116384 | 87182 | 25922 | 6567 | 2213 | . 28 2016 | 강원도 | 616346 | 197917 | 180639 | 117408 | 86362 | 25441 | 6477 | 2102 | . 46 2017 | 강원도 | 620729 | 199645 | 187312 | 117912 | 83773 | 24178 | 6020 | 1889 | . 64 2018 | 강원도 | 628484 | 206295 | 193105 | 117713 | 81241 | 22835 | 5522 | 1773 | . 82 2019 | 강원도 | 633942 | 208857 | 200035 | 117416 | 79476 | 21560 | 5043 | 1555 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29 2016 | 충청북도 | 617914 | 187377 | 170184 | 121917 | 99555 | 29534 | 7075 | 2272 | . 47 2017 | 충청북도 | 629073 | 195186 | 176617 | 122463 | 97422 | 28487 | 6737 | 2161 | . 65 2018 | 충청북도 | 640978 | 204109 | 183501 | 123042 | 94970 | 27286 | 6166 | 1904 | . 83 2019 | 충청북도 | 654713 | 215196 | 191655 | 123239 | 91764 | 25674 | 5503 | 1682 | . 101 2020 | 충청북도 | 678922 | 236208 | 198840 | 122347 | 91329 | 24143 | 4694 | 1361 | . 108 rows × 10 columns . house[house[&#39;시점&#39;] == 2020 ].sort_values(by = [ &#39;일반가구_계&#39;], ascending=False) #2020년 데이터만 선택, 내림차순 . 시점 행정구역별(읍면동) 일반가구_계 1인 2인 3인 4인 5인 6인 7인 이상 . 90 2020 | 전국 | 20926710 | 6643354 | 5864525 | 4200629 | 3271315 | 761417 | 147172 | 38298 | . 99 2020 | 경기도 | 5098431 | 1406010 | 1350139 | 1119823 | 951370 | 218173 | 42094 | 10822 | . 91 2020 | 서울특별시 | 3982290 | 1390701 | 1033901 | 792690 | 602791 | 130122 | 25770 | 6315 | . 92 2020 | 부산광역시 | 1405037 | 455207 | 411455 | 282233 | 203769 | 42608 | 7874 | 1891 | . 106 2020 | 경상남도 | 1350155 | 417737 | 399700 | 270061 | 205658 | 46516 | 8410 | 2073 | . 94 2020 | 인천광역시 | 1147200 | 324841 | 316387 | 251928 | 198528 | 44949 | 8440 | 2127 | . 105 2020 | 경상북도 | 1131819 | 388791 | 363061 | 202539 | 137092 | 32174 | 6388 | 1774 | . 93 2020 | 대구광역시 | 985816 | 304543 | 276237 | 205048 | 159654 | 33131 | 5976 | 1227 | . 102 2020 | 충청남도 | 892222 | 304973 | 264909 | 159754 | 121589 | 32419 | 6618 | 1960 | . 104 2020 | 전라남도 | 761518 | 256633 | 251506 | 131372 | 87883 | 26561 | 5727 | 1836 | . 103 2020 | 전라북도 | 755575 | 255269 | 233334 | 134414 | 96907 | 27962 | 5920 | 1769 | . 101 2020 | 충청북도 | 678922 | 236208 | 198840 | 122347 | 91329 | 24143 | 4694 | 1361 | . 100 2020 | 강원도 | 661039 | 231371 | 206755 | 117504 | 79073 | 20669 | 4409 | 1258 | . 96 2020 | 대전광역시 | 631208 | 228842 | 164795 | 116989 | 93698 | 21883 | 4034 | 967 | . 95 2020 | 광주광역시 | 599217 | 193948 | 162403 | 115978 | 97323 | 24613 | 4011 | 941 | . 97 2020 | 울산광역시 | 444087 | 122848 | 123591 | 99739 | 79489 | 15447 | 2398 | 575 | . 107 2020 | 제주특별자치도 | 263068 | 81855 | 74308 | 49903 | 38747 | 13719 | 3380 | 1156 | . 98 2020 | 세종특별자치시 | 139106 | 43577 | 33204 | 28307 | 26415 | 6328 | 1029 | 246 | . . &#54665;&#44284; &#50676; &#49325;&#51228;&#54616;&#44592; . house.drop(columns = [&quot;일반가구_계&quot;], inplace= True) house . 시점 행정구역별(읍면동) 1인 2인 3인 4인 5인 6인 7인 이상 . 0 2015 | 전국 | 5203440 | 4993818 | 4100979 | 3588931 | 940413 | 217474 | 65975 | . 1 2015 | 서울특별시 | 1115744 | 930467 | 817440 | 701945 | 169436 | 38547 | 10911 | . 2 2015 | 부산광역시 | 361749 | 366048 | 296742 | 238031 | 56971 | 12928 | 3431 | . 3 2015 | 대구광역시 | 239517 | 239824 | 208795 | 184258 | 44071 | 9582 | 2481 | . 4 2015 | 인천광역시 | 243678 | 265079 | 245135 | 220538 | 55230 | 12253 | 3504 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 103 2020 | 전라북도 | 255269 | 233334 | 134414 | 96907 | 27962 | 5920 | 1769 | . 104 2020 | 전라남도 | 256633 | 251506 | 131372 | 87883 | 26561 | 5727 | 1836 | . 105 2020 | 경상북도 | 388791 | 363061 | 202539 | 137092 | 32174 | 6388 | 1774 | . 106 2020 | 경상남도 | 417737 | 399700 | 270061 | 205658 | 46516 | 8410 | 2073 | . 107 2020 | 제주특별자치도 | 81855 | 74308 | 49903 | 38747 | 13719 | 3380 | 1156 | . 108 rows × 9 columns . house.drop(index = house[house[&quot;행정구역별(읍면동)&quot;] == &#39;전국&#39;].index, inplace= True ) house.head(20) #특정 열에 있는 &quot;전국&quot; 문자열을 가지는 행(.index)을 전부 drop하기 . 시점 행정구역별(읍면동) 1인 2인 3인 4인 5인 6인 7인 이상 . 1 2015 | 서울특별시 | 1115744 | 930467 | 817440 | 701945 | 169436 | 38547 | 10911 | . 2 2015 | 부산광역시 | 361749 | 366048 | 296742 | 238031 | 56971 | 12928 | 3431 | . 3 2015 | 대구광역시 | 239517 | 239824 | 208795 | 184258 | 44071 | 9582 | 2481 | . 4 2015 | 인천광역시 | 243678 | 265079 | 245135 | 220538 | 55230 | 12253 | 3504 | . 5 2015 | 광주광역시 | 163577 | 137662 | 115701 | 109612 | 32199 | 6647 | 1759 | . 6 2015 | 대전광역시 | 169391 | 140603 | 122088 | 112055 | 30022 | 6489 | 1856 | . 7 2015 | 울산광역시 | 103551 | 104101 | 100230 | 90735 | 19914 | 3851 | 1030 | . 8 2015 | 세종특별자치시 | 21889 | 18010 | 15219 | 14560 | 4238 | 994 | 309 | . 9 2015 | 경기도 | 1026471 | 1062222 | 1005330 | 971146 | 246638 | 56117 | 16818 | . 10 2015 | 강원도 | 189379 | 178470 | 116384 | 87182 | 25922 | 6567 | 2213 | . 11 2015 | 충청북도 | 173598 | 167146 | 120634 | 100527 | 30110 | 7428 | 2413 | . 12 2015 | 충청남도 | 234513 | 226658 | 153585 | 128382 | 39334 | 10031 | 3682 | . 13 2015 | 전라북도 | 213750 | 209008 | 135856 | 110132 | 36433 | 9038 | 3094 | . 14 2015 | 전라남도 | 218864 | 227975 | 131454 | 96940 | 33229 | 8881 | 3269 | . 15 2015 | 경상북도 | 322569 | 320498 | 207395 | 156364 | 42161 | 10360 | 3377 | . 16 2015 | 경상남도 | 346754 | 341745 | 265127 | 228216 | 59248 | 13383 | 4014 | . 17 2015 | 제주특별자치도 | 58446 | 58302 | 43864 | 38308 | 15257 | 4378 | 1814 | . 19 2016 | 서울특별시 | 1138860 | 931262 | 816946 | 686469 | 163555 | 37153 | 10460 | . 20 2016 | 부산광역시 | 372412 | 370623 | 296627 | 233160 | 55496 | 12460 | 3392 | . 21 2016 | 대구광역시 | 247444 | 242931 | 209737 | 181468 | 42748 | 9114 | 2311 | . house.reset_index(drop=True,inplace=True) #drop=True는 행 인덱스를 재구성 할 때 인덱스를 나타내는 새로운 열을 만들지 말라는 명령이다. house.head(20) . index 시점 행정구역별(읍면동) 1인 2인 3인 4인 5인 6인 7인 이상 . 0 0 | 2015 | 서울특별시 | 1115744 | 930467 | 817440 | 701945 | 169436 | 38547 | 10911 | . 1 1 | 2015 | 부산광역시 | 361749 | 366048 | 296742 | 238031 | 56971 | 12928 | 3431 | . 2 2 | 2015 | 대구광역시 | 239517 | 239824 | 208795 | 184258 | 44071 | 9582 | 2481 | . 3 3 | 2015 | 인천광역시 | 243678 | 265079 | 245135 | 220538 | 55230 | 12253 | 3504 | . 4 4 | 2015 | 광주광역시 | 163577 | 137662 | 115701 | 109612 | 32199 | 6647 | 1759 | . 5 5 | 2015 | 대전광역시 | 169391 | 140603 | 122088 | 112055 | 30022 | 6489 | 1856 | . 6 6 | 2015 | 울산광역시 | 103551 | 104101 | 100230 | 90735 | 19914 | 3851 | 1030 | . 7 7 | 2015 | 세종특별자치시 | 21889 | 18010 | 15219 | 14560 | 4238 | 994 | 309 | . 8 8 | 2015 | 경기도 | 1026471 | 1062222 | 1005330 | 971146 | 246638 | 56117 | 16818 | . 9 9 | 2015 | 강원도 | 189379 | 178470 | 116384 | 87182 | 25922 | 6567 | 2213 | . 10 10 | 2015 | 충청북도 | 173598 | 167146 | 120634 | 100527 | 30110 | 7428 | 2413 | . 11 11 | 2015 | 충청남도 | 234513 | 226658 | 153585 | 128382 | 39334 | 10031 | 3682 | . 12 12 | 2015 | 전라북도 | 213750 | 209008 | 135856 | 110132 | 36433 | 9038 | 3094 | . 13 13 | 2015 | 전라남도 | 218864 | 227975 | 131454 | 96940 | 33229 | 8881 | 3269 | . 14 14 | 2015 | 경상북도 | 322569 | 320498 | 207395 | 156364 | 42161 | 10360 | 3377 | . 15 15 | 2015 | 경상남도 | 346754 | 341745 | 265127 | 228216 | 59248 | 13383 | 4014 | . 16 16 | 2015 | 제주특별자치도 | 58446 | 58302 | 43864 | 38308 | 15257 | 4378 | 1814 | . 17 17 | 2016 | 서울특별시 | 1138860 | 931262 | 816946 | 686469 | 163555 | 37153 | 10460 | . 18 18 | 2016 | 부산광역시 | 372412 | 370623 | 296627 | 233160 | 55496 | 12460 | 3392 | . 19 19 | 2016 | 대구광역시 | 247444 | 242931 | 209737 | 181468 | 42748 | 9114 | 2311 | . &#50676;&#51060;&#47492; &#48148;&#44984;&#44592; . house.rename( columns={ &quot;시점&quot; : &quot;year&quot;, &quot;행정구역별(읍면동)&quot; : &quot;region&quot;, &quot;1인&quot; : &quot;p1&quot;, &quot;2인&quot; : &quot;p2&quot;, &quot;3인&quot; : &quot;p3&quot;, &quot;4인&quot; : &quot;p4&quot;, &quot;5인&quot; : &quot;p5&quot;, &quot;6인&quot; : &quot;p6&quot;, &quot;7인 이상&quot; : &quot;p7plus&quot;}, inplace=True) house . house.rename(columns = { &quot;1인&quot; : &quot;p1&quot;, &quot;2인&quot; : &quot;p2&quot;}) . index 시점 행정구역별(읍면동) p1 p2 3인 4인 5인 6인 7인 이상 . 0 0 | 2015 | 서울특별시 | 1115744 | 930467 | 817440 | 701945 | 169436 | 38547 | 10911 | . 1 1 | 2015 | 부산광역시 | 361749 | 366048 | 296742 | 238031 | 56971 | 12928 | 3431 | . 2 2 | 2015 | 대구광역시 | 239517 | 239824 | 208795 | 184258 | 44071 | 9582 | 2481 | . 3 3 | 2015 | 인천광역시 | 243678 | 265079 | 245135 | 220538 | 55230 | 12253 | 3504 | . 4 4 | 2015 | 광주광역시 | 163577 | 137662 | 115701 | 109612 | 32199 | 6647 | 1759 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 97 97 | 2020 | 전라북도 | 255269 | 233334 | 134414 | 96907 | 27962 | 5920 | 1769 | . 98 98 | 2020 | 전라남도 | 256633 | 251506 | 131372 | 87883 | 26561 | 5727 | 1836 | . 99 99 | 2020 | 경상북도 | 388791 | 363061 | 202539 | 137092 | 32174 | 6388 | 1774 | . 100 100 | 2020 | 경상남도 | 417737 | 399700 | 270061 | 205658 | 46516 | 8410 | 2073 | . 101 101 | 2020 | 제주특별자치도 | 81855 | 74308 | 49903 | 38747 | 13719 | 3380 | 1156 | . 102 rows × 10 columns . &#50836;&#50557; . 정렬할 때는 sort_values() 메소드를 사용한다. . 열 또는 행을 제거할 때 drop() 메소드를 사용한다. . 열이름을 바꿀 때는 rename() 메소드를 사용한다. . 행의 인덱스를 재구성하는 경우 reset_index() 를 이용한다. . 변경된 내용을 데이터프레임에 적용하려면 inplace = True 선택문을 사용한다. . 4-3 . weather = pd.read_csv(&quot;https://ilovedata.github.io/teaching/bigdata2/data/weather_stats.csv&quot; , encoding=&quot;cp949&quot;) weather . 일시 평균기온(℃) 최고기온 평균(℃) 최저기온 평균(℃) 강수량(mm) . 0 2020년 | 13.2 | 17.9 | 9.4 | 1651.1 | . 1 2019년 | 13.5 | 18.5 | 9.3 | 891.3 | . 2 2018년 | 12.9 | 17.9 | 8.8 | 1284.1 | . 3 2017년 | 13.0 | 18.1 | 8.8 | 1233.2 | . 4 2016년 | 13.6 | 18.5 | 9.4 | 991.7 | . weather.columns = [&quot;year&quot;, &quot;avg_t&quot;, &quot;max_t&quot;, &quot;min_t&quot;, &quot;rain&quot;] #열 이름 한 꺼번에 다 바꾸고 싶을 때 사용하기 . 화씨(F)로 된 새로운 열 avg_t_F를 만들어 보자. . 𝐹=32+1.8×𝐶 . weather[&quot;avg_t_F&quot;] = 32 + 1.8*weather[&quot;avg_t&quot;] weather . year avg_t max_t min_t rain avg_t_F . 0 2020년 | 13.2 | 17.9 | 9.4 | 1651.1 | 55.76 | . 1 2019년 | 13.5 | 18.5 | 9.3 | 891.3 | 56.30 | . 2 2018년 | 12.9 | 17.9 | 8.8 | 1284.1 | 55.22 | . 3 2017년 | 13.0 | 18.1 | 8.8 | 1233.2 | 55.40 | . 4 2016년 | 13.6 | 18.5 | 9.4 | 991.7 | 56.48 | . weather[&quot;avg_t_minmix&quot;] = (weather[&quot;max_t&quot;] + weather[&quot;min_t&quot;])/2.0 weather[&quot;avg_t_minmix_2&quot;] = weather[ [&quot;max_t&quot;, &quot;min_t&quot;] ].sum(axis=1) /2.0 weather . year avg_t max_t min_t rain avg_t_F avg_t_minmix avg_t_minmix_2 . 0 2020년 | 13.2 | 17.9 | 9.4 | 1651.1 | 55.76 | 13.65 | 13.65 | . 1 2019년 | 13.5 | 18.5 | 9.3 | 891.3 | 56.30 | 13.90 | 13.90 | . 2 2018년 | 12.9 | 17.9 | 8.8 | 1284.1 | 55.22 | 13.35 | 13.35 | . 3 2017년 | 13.0 | 18.1 | 8.8 | 1233.2 | 55.40 | 13.45 | 13.45 | . 4 2016년 | 13.6 | 18.5 | 9.4 | 991.7 | 56.48 | 13.95 | 13.95 | . weather[ [&quot;max_t&quot;, &quot;min_t&quot;] ].sum(axis=0) #열(0) 1:행 . max_t 90.9 min_t 45.7 dtype: float64 . &#50676;&#51032; &#50836;&#50557; &#53685;&#44228; . weather.mean(axis=0,numeric_only=True) #mean(axis=0) 은 문자로 구성된 열을 자동적으로 제외하고 각 열의 평균을 구해준다 . avg_t 13.240 max_t 18.180 min_t 9.140 rain 1210.280 avg_t_F 55.832 avg_t_minmix 13.660 avg_t_minmix_2 13.660 dtype: float64 . weather.mean(axis=0,numeric_only=True).to_frame() #데이터 프레임으로 만들기 . 0 . avg_t 13.240 | . max_t 18.180 | . min_t 9.140 | . rain 1210.280 | . avg_t_F 55.832 | . avg_t_minmix 13.660 | . avg_t_minmix_2 13.660 | . weather.mean(axis=0,numeric_only=True).to_frame().reset_index().rename(columns={0:&quot;mean&quot;}) . index mean . 0 avg_t | 13.240 | . 1 max_t | 18.180 | . 2 min_t | 9.140 | . 3 rain | 1210.280 | . 4 avg_t_F | 55.832 | . 5 avg_t_minmix | 13.660 | . 6 avg_t_minmix_2 | 13.660 | . house = pd.read_csv(&quot;https://ilovedata.github.io/teaching/bigdata2/data/house_final.csv&quot;) . #groupby( by=...)는 데이터프레임을 물리적으로 그룹을 나누어 주는 것은 아니고 메소드에서 지정된 그룹을 데이터프레임에 적용하는 기능을 한다. house_grp = house.groupby( by=[&quot;year&quot;] ) . house_year = house_grp.sum().reset_index() house_year . year p1 p2 p3 p4 p5 p6 p7plus . 0 2015 | 5203440 | 4993818 | 4100979 | 3588931 | 940413 | 217474 | 65975 | . 1 2016 | 5397615 | 5067166 | 4151701 | 3551410 | 924373 | 211475 | 63956 | . 2 2017 | 5618677 | 5260332 | 4178641 | 3473897 | 886479 | 197517 | 58332 | . 3 2018 | 5848594 | 5445691 | 4203792 | 3396320 | 849167 | 182886 | 52738 | . 4 2019 | 6147516 | 5663330 | 4217736 | 3300114 | 801048 | 166866 | 46578 | . 5 2020 | 6643354 | 5864525 | 4200629 | 3271315 | 761417 | 147172 | 38298 | . house_year[&quot;total&quot;] = house_year.sum(axis=1) house_year . year p1 p2 p3 p4 p5 p6 p7plus total . 0 2015 | 5203440 | 4993818 | 4100979 | 3588931 | 940413 | 217474 | 65975 | 19113045 | . 1 2016 | 5397615 | 5067166 | 4151701 | 3551410 | 924373 | 211475 | 63956 | 19369712 | . 2 2017 | 5618677 | 5260332 | 4178641 | 3473897 | 886479 | 197517 | 58332 | 19675892 | . 3 2018 | 5848594 | 5445691 | 4203792 | 3396320 | 849167 | 182886 | 52738 | 19981206 | . 4 2019 | 6147516 | 5663330 | 4217736 | 3300114 | 801048 | 166866 | 46578 | 20345207 | . 5 2020 | 6643354 | 5864525 | 4200629 | 3271315 | 761417 | 147172 | 38298 | 20928730 | . house_year[&quot;total&quot;] = house_year[ [&quot;p1&quot;,&quot;p2&quot;,&quot;p3&quot;,&quot;p4&quot;,&quot;p5&quot;,&quot;p6&quot;,&quot;p7plus&quot;] ].sum(axis=1) house_year . year p1 p2 p3 p4 p5 p6 p7plus total . 0 2015 | 5203440 | 4993818 | 4100979 | 3588931 | 940413 | 217474 | 65975 | 19111030 | . 1 2016 | 5397615 | 5067166 | 4151701 | 3551410 | 924373 | 211475 | 63956 | 19367696 | . 2 2017 | 5618677 | 5260332 | 4178641 | 3473897 | 886479 | 197517 | 58332 | 19673875 | . 3 2018 | 5848594 | 5445691 | 4203792 | 3396320 | 849167 | 182886 | 52738 | 19979188 | . 4 2019 | 6147516 | 5663330 | 4217736 | 3300114 | 801048 | 166866 | 46578 | 20343188 | . 5 2020 | 6643354 | 5864525 | 4200629 | 3271315 | 761417 | 147172 | 38298 | 20926710 | . house_year[&#39;p1_percent&#39;] = house_year[&#39;p1&#39;] / house_year[&#39;total&#39;] *100 house_year . year p1 p2 p3 p4 p5 p6 p7plus total p1_percent . 0 2015 | 5203440 | 4993818 | 4100979 | 3588931 | 940413 | 217474 | 65975 | 19111030 | 27.227418 | . 1 2016 | 5397615 | 5067166 | 4151701 | 3551410 | 924373 | 211475 | 63956 | 19367696 | 27.869164 | . 2 2017 | 5618677 | 5260332 | 4178641 | 3473897 | 886479 | 197517 | 58332 | 19673875 | 28.559076 | . 3 2018 | 5848594 | 5445691 | 4203792 | 3396320 | 849167 | 182886 | 52738 | 19979188 | 29.273432 | . 4 2019 | 6147516 | 5663330 | 4217736 | 3300114 | 801048 | 166866 | 46578 | 20343188 | 30.219039 | . 5 2020 | 6643354 | 5864525 | 4200629 | 3271315 | 761417 | 147172 | 38298 | 20926710 | 31.745812 | . house_year.plot.line(x=&#39;year&#39;, y=&#39;p1_percent&#39;) . &lt;AxesSubplot:xlabel=&#39;year&#39;&gt; . df = pd.DataFrame( { &quot;school&quot; : [&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;], &quot;sex&quot; : [&quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;], &quot;score&quot; : [100, 98, 34, 83, 56, 90, 65] }) df . school sex score . 0 A | M | 100 | . 1 A | M | 98 | . 2 A | F | 34 | . 3 A | F | 83 | . 4 B | M | 56 | . 5 B | F | 90 | . 6 B | F | 65 | . . df.groupby([&quot;school&quot;, &quot;sex&quot;]).mean().reset_index() . school sex score . 0 A | F | 58.5 | . 1 A | M | 99.0 | . 2 B | F | 77.5 | . 3 B | M | 56.0 | . &quot;p1&quot;.startswith(&#39;p&#39;) . True . &quot;year&quot;.startswith(&#39;p&#39;) . False . [ x for x in house_year.columns if x.startswith(&#39;p&#39;) ] . [&#39;p1&#39;, &#39;p2&#39;, &#39;p3&#39;, &#39;p4&#39;, &#39;p5&#39;, &#39;p6&#39;, &#39;p7plus&#39;, &#39;p1_percent&#39;] . &#45936;&#51060;&#53552; &#54532;&#47112;&#51076; &#44208;&#54633; . df1 = pd.DataFrame({ &quot;name&quot; : [&quot;철수&quot;, &quot;영이&quot;, &quot;John&quot;], &quot;age&quot; : [23, 34, 19] }) df1 . name age . 0 철수 | 23 | . 1 영이 | 34 | . 2 John | 19 | . df2 = pd.DataFrame({ &quot;name&quot; : [&quot;철수&quot;, &quot;영이&quot;, &quot;John&quot;], &quot;sex&quot; : [&quot;M&quot;, &quot;F&quot;, &quot;M&quot;] }) df2 . name sex . 0 철수 | M | . 1 영이 | F | . 2 John | M | . import numpy as np import pandas as pd . np.concatenate([df1, df2], axis = 0) . array([[&#39;철수&#39;, 23], [&#39;영이&#39;, 34], [&#39;John&#39;, 19], [&#39;철수&#39;, &#39;M&#39;], [&#39;영이&#39;, &#39;F&#39;], [&#39;John&#39;, &#39;M&#39;]], dtype=object) . pd.merge(df1, df2, on=&quot;name&quot;) . name age sex . 0 철수 | 23 | M | . 1 영이 | 34 | F | . 2 John | 19 | M | . df3 = pd.DataFrame({ &quot;name&quot; : [&quot;철수&quot;, &quot;영이&quot;], &quot;weight&quot; : [55, 44] }) df3 . name weight . 0 철수 | 55 | . 1 영이 | 44 | . pd.merge(df1, df3, on=&quot;name&quot;) #식별자, 조건을 안 주면 공통된 식별자만 데이터 프레임으로 만든다 . name age weight . 0 철수 | 23 | 55 | . 1 영이 | 34 | 44 | . pd.merge(df1, df3, on=&quot;name&quot;, how=&#39;left&#39;) #df1(왼쪽 데이터 자료)에 있는 식별자 다 뽑음 . name age weight . 0 철수 | 23 | 55.0 | . 1 영이 | 34 | 44.0 | . 2 John | 19 | NaN | . df4 = pd.DataFrame({ &quot;name&quot; : [&quot;철수&quot;, &quot;영이&quot;, &quot;흥민&quot;], &quot;height&quot; : [167, 175, 183] }) df4 . name height . 0 철수 | 167 | . 1 영이 | 175 | . 2 흥민 | 183 | . pd.merge(df1, df4, on=&quot;name&quot;, how=&#39;right&#39;) # 식별자는 오른쪽 데이터프레임만 있는 것으로 . name age height . 0 철수 | 23.0 | 167 | . 1 영이 | 34.0 | 175 | . 2 흥민 | NaN | 183 | . pd.merge(df1, df4, on=&quot;name&quot;, how=&#39;inner&#39;) # 식별자는 두 데이터프레임에 공통인 것 . name age height . 0 철수 | 23 | 167 | . 1 영이 | 34 | 175 | . pd.merge(df1, df4, on=&quot;name&quot;, how=&#39;outer&#39;) # 식별자는 두 데이터프레임의 모든 것 . name age height . 0 철수 | 23.0 | 167.0 | . 1 영이 | 34.0 | 175.0 | . 2 John | 19.0 | NaN | . 3 흥민 | NaN | 183.0 | . house = pd.read_csv(&quot;https://ilovedata.github.io/teaching/bigdata2/data/korea_house_data_01.csv&quot;, encoding=&quot;CP949&quot;) . house.head(5) . 시점 행정구역별(읍면동) 일반가구_계 1인 2인 3인 4인 5인 6인 7인 이상 . 0 2015 | 전국 | 19111030 | 5203440 | 4993818 | 4100979 | 3588931 | 940413 | 217474 | 65975 | . 1 2015 | 서울특별시 | 3784490 | 1115744 | 930467 | 817440 | 701945 | 169436 | 38547 | 10911 | . 2 2015 | 부산광역시 | 1335900 | 361749 | 366048 | 296742 | 238031 | 56971 | 12928 | 3431 | . 3 2015 | 대구광역시 | 928528 | 239517 | 239824 | 208795 | 184258 | 44071 | 9582 | 2481 | . 4 2015 | 인천광역시 | 1045417 | 243678 | 265079 | 245135 | 220538 | 55230 | 12253 | 3504 | . . house.drop(columns = [&quot;일반가구_계&quot;], inplace = True) house.drop(index = house[house[&quot;행정구역별(읍면동)&quot;] == &quot;전국&quot;].index, inplace = True) . house.rename( columns={ &quot;시점&quot; : &quot;year&quot;, &quot;행정구역별(읍면동)&quot; : &quot;region&quot;, &quot;1인&quot; : &quot;p1&quot;, &quot;2인&quot; : &quot;p2&quot;, &quot;3인&quot; : &quot;p3&quot;, &quot;4인&quot; : &quot;p4&quot;, &quot;5인&quot; : &quot;p5&quot;, &quot;6인&quot; : &quot;p6&quot;, &quot;7인 이상&quot; : &quot;p7plus&quot;}, inplace=True) . house.head(5) . year region p1 p2 p3 p4 p5 p6 p7plus . 1 2015 | 서울특별시 | 1115744 | 930467 | 817440 | 701945 | 169436 | 38547 | 10911 | . 2 2015 | 부산광역시 | 361749 | 366048 | 296742 | 238031 | 56971 | 12928 | 3431 | . 3 2015 | 대구광역시 | 239517 | 239824 | 208795 | 184258 | 44071 | 9582 | 2481 | . 4 2015 | 인천광역시 | 243678 | 265079 | 245135 | 220538 | 55230 | 12253 | 3504 | . 5 2015 | 광주광역시 | 163577 | 137662 | 115701 | 109612 | 32199 | 6647 | 1759 | . house_final =house.groupby( by=[&quot;year&quot;] ).sum().reset_index() . house_final[&#39;house_total&#39;] = house_final[ [ &quot;p1&quot;,&quot;p2&quot;,&quot;p3&quot;,&quot;p4&quot;,&quot;p5&quot;,&quot;p6&quot;,&quot;p7plus&quot;] ].sum(axis=1) . house_final . year p1 p2 p3 p4 p5 p6 p7plus house_total . 0 2015 | 5203440 | 4993818 | 4100979 | 3588931 | 940413 | 217474 | 65975 | 19111030 | . 1 2016 | 5397615 | 5067166 | 4151701 | 3551410 | 924373 | 211475 | 63956 | 19367696 | . 2 2017 | 5618677 | 5260332 | 4178641 | 3473897 | 886479 | 197517 | 58332 | 19673875 | . 3 2018 | 5848594 | 5445691 | 4203792 | 3396320 | 849167 | 182886 | 52738 | 19979188 | . 4 2019 | 6147516 | 5663330 | 4217736 | 3300114 | 801048 | 166866 | 46578 | 20343188 | . 5 2020 | 6643354 | 5864525 | 4200629 | 3271315 | 761417 | 147172 | 38298 | 20926710 | . pop = pd.read_csv(&quot;https://ilovedata.github.io/teaching/bigdata2/data/korea_population_data_01.csv&quot;, encoding=&quot;CP949&quot;) . print(pop) . 시점 행정구역별(읍면동) 남자(명) 여자(명) 0 2015 서울특별시 4859535 5044777 1 2015 부산광역시 1701347 1747390 2 2015 대구광역시 1228511 1237541 3 2015 인천광역시 1455017 1435434 4 2015 광주광역시 748867 754014 .. ... ... ... ... 97 2020 전라북도 899058 903708 98 2020 전라남도 903281 885526 99 2020 경상북도 1338823 1305934 100 2020 경상남도 1692212 1640844 101 2020 제주특별자치도 338173 332685 [102 rows x 4 columns] . pop.rename( columns={ &quot;시점&quot; : &quot;year&quot;, &quot;행정구역별(읍면동)&quot; : &quot;region&quot;, &quot;남자(명)&quot; : &quot;male&quot;, &quot;여자(명)&quot; : &quot;female&quot;}, inplace=True) . . pop_final = pop.groupby(&quot;year&quot;).sum().reset_index() #연도별로 합을 구한 걸 피날에다가 저장 -&gt; 그 다음으로 토탈 열을 구하는 것을 해야함 . print(pop_final) #sum 함수를 사용하면 문자열을 배제해줌 . year male female 0 2015 25608502 25460873 1 2016 25696987 25572567 2 2017 25768055 25654452 3 2018 25877195 25752317 4 2019 25952070 25827133 5 2020 25915207 25913929 . pop_final[&quot;pop_total&quot;] = pop_final[[&quot;male&quot;, &quot;female&quot;]].sum(axis=1) . pop_final . year male female pop_total . 0 2015 | 25608502 | 25460873 | 51069375 | . 1 2016 | 25696987 | 25572567 | 51269554 | . 2 2017 | 25768055 | 25654452 | 51422507 | . 3 2018 | 25877195 | 25752317 | 51629512 | . 4 2019 | 25952070 | 25827133 | 51779203 | . 5 2020 | 25915207 | 25913929 | 51829136 | . all_final = pd.merge(house_final[[&#39;year&#39;, &#39;house_total&#39;]], pop_final[[&#39;year&#39;,&#39;pop_total&#39;]], on = &#39;year&#39;) . all_final . year house_total pop_total . 0 2015 | 19111030 | 51069375 | . 1 2016 | 19367696 | 51269554 | . 2 2017 | 19673875 | 51422507 | . 3 2018 | 19979188 | 51629512 | . 4 2019 | 20343188 | 51779203 | . 5 2020 | 20926710 | 51829136 | . all_final[&quot;avg_person_per_house&quot;] = all_final[&quot;pop_total&quot;] / all_final[&quot;house_total&quot;] . all_final.plot.line(x=&#39;year&#39;, y=&#39;avg_person_per_house&#39;) . &lt;AxesSubplot:xlabel=&#39;year&#39;&gt; . &#52309;&#53552; 5 . import matplotlib.pyplot as plt from matplotlib import rc %matplotlib inline plt.rc(&#39;font&#39;, family=&#39;NanumBarunGothic&#39;) # clolab 에서 한글 사용 plt.rcParams[&#39;axes.unicode_minus&#39;] = False plt.rcParams[&quot;figure.figsize&quot;] = (8,8) # 그림 크기 조정 import pandas as pd . physical_data = pd.read_csv(&quot;https://ilovedata.github.io/teaching/bigdata2/data/physical_test_2018_data.csv&quot;,sep=&#39;,&#39;, encoding = &#39;utf-8-sig&#39;) . physical_names = pd.read_csv(&quot;https://ilovedata.github.io/teaching/bigdata2/data/physical_test_2018_names.csv&quot;, sep=&#39;,&#39;, encoding = &#39;utf-8-sig&#39;) . physical_data.loc[:,&quot;CENTER_NM&quot;] = physical_data.loc[:,&quot;CENTER_NM&quot;].astype(&quot;category&quot;) physical_data.loc[:,&quot;CERT_GBN&quot;] = physical_data.loc[:, &quot;CERT_GBN&quot;].astype(&quot;category&quot;) physical_data.loc[:,&quot;AGE_GBN&quot;] = physical_data.loc[:, &quot;AGE_GBN&quot;].astype(&quot;category&quot;) physical_data.loc[:,&quot;TEST_SEX&quot; ] = physical_data.loc[:, &quot;TEST_SEX&quot;].astype(&quot;category&quot;) . physical_data[&quot;TEST_SEX&quot;].value_counts() . M 1081 F 793 Name: TEST_SEX, dtype: int64 . physical_data[&quot;TEST_SEX&quot;].value_counts().plot.pie() . &lt;AxesSubplot:ylabel=&#39;TEST_SEX&#39;&gt; . findfont: Font family [&#39;NanumBarunGothic&#39;] not found. Falling back to DejaVu Sans. . . import seaborn as sns sns.scatterplot(data=physical_data, x=&#39;ITEM_F001&#39;,y= &#39;ITEM_F002&#39;,hue=&#39;TEST_SEX&#39;, alpha=0.5) . &lt;AxesSubplot:xlabel=&#39;ITEM_F001&#39;, ylabel=&#39;ITEM_F002&#39;&gt; . sns.displot(data=physical_data, x=&quot;ITEM_F002&quot;, hue=&quot;TEST_SEX&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fbbf32ceb80&gt; . sns.catplot(data=physical_data, x=&#39;TEST_SEX&#39;,y= &#39;ITEM_F002&#39;, kind=&quot;box&quot; ) . &lt;seaborn.axisgrid.FacetGrid at 0x7fbbe3da0520&gt; . import pandas as pd import seaborn as sns import numpy as np from scipy.stats import norm # 정규분포 import matplotlib.pyplot as plt # 그래프 그리기를 위한 라이브러리 .",
            "url": "https://g-gg-ggg.github.io/Oops/python/2022/10/26/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%9C%EB%A1%A0-4,-5-,-sortvalues,-merge(on-=,-how-),-%ED%96%89%EC%97%B4%EC%82%AD%EC%A0%9C,-sns,.html",
            "relUrl": "/python/2022/10/26/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%EA%B0%9C%EB%A1%A0-4,-5-,-sortvalues,-merge(on-=,-how-),-%ED%96%89%EC%97%B4%EC%82%AD%EC%A0%9C,-sns,.html",
            "date": " • Oct 26, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "빅데이터 개론 복습1",
            "content": "2-1 &#54364;&#54788;&#49885;&#44284; &#48320;&#49688; . hour_rate = 10000 month_working_days = 20 day_working_hours = 4 month_pay = hour_rate * month_working_days * day_working_hours month_pay . 800000 . text_2 =&#39;you can &#39;t come&#39; #역슬래시로 구분하기 text_2 . &#34;you can&#39;t come&#34; . print(&quot;첫 번째 줄 n두 번째 줄&quot;) . 첫 번째 줄 두 번째 줄 . text_4 = &#39;C: some name&#39; print(text_4) . C: some ame . text_5 = r&#39;C: some name&#39; print(text_5) . C: some name . 라이브러리와 함수 | . import math . math.sin(2) . 0.9092974268256817 . math.log(100) . 4.605170185988092 . 리스트 | . list_1 = [1, 2, 0.5, &#39;chair&#39;] list_1 . [1, 2, 0.5, &#39;chair&#39;] . list_2 = [ list_1, &quot;A&quot;, [1,2,3] ] list_2 . [[1, 2, 0.5, &#39;chair&#39;], &#39;A&#39;, [1, 2, 3]] . 파이썬의 인덱스는 1이 아닌 0 부터 시작하므로 이를 언제나 유념해야 한다 . squares = [1, 4, 9, 16, 25] . squares[0] . 1 . squares[4] . 25 . squares[:1] . [1] . squares[:-1] . [1, 4, 9, 16] . squares[-1:] . [25] . 1 in [1, 2, 3, 4] . True . a = 5 a in [1, 2, 3, 4, 5] . True . &#49892;&#49845; . hstudent.csv&#39;는 20명의 고등학생 성별, 학년, 키(cm), 몸무게(kg)를 조사한 자료이다. | 체질량지수는 몸무게(kg)를 키의 제곱(m)으로 나눈 값입니다. 5번 학생의 체질량지수는? | 10번 학생의 체질량지수는? | . | 학생의 키와 몸무게를 각각 height와 weight에 저장하고, height와 weight를 사용하여 체질량지수 계산 5번 과 10번 학생 각각에 대하여 계산한 후 bmi5와 bmi10에 저장 | bmi5와 bmi10의 크기 비교 결과 print함수를 사용하여 보여주기 | . | 1번부터 5번까지의 학생 키를 원소로 가지는 리스트 변수 height1_5생성 1번부터 3번까지 학생의 키 추출 | . | . 나는 전북대학교 학과 이름 입니다. (예, 나는 전북대학교 통계학과 최혜미입니다) 위 문자열을 값으로 가지는 변수 name_dept를 생성 | 변수 name_dept에서 전북대학교 추출 | . | df = pd.read_csv(&quot;hstudent.csv&quot;) df . gender grade height weight . 0 1 | 3 | 183 | 82 | . 1 2 | 1 | 168 | 52 | . 2 2 | 1 | 160 | 48 | . 3 2 | 2 | 160 | 50 | . 4 1 | 1 | 160 | 79 | . 5 1 | 2 | 180 | 73 | . 6 2 | 2 | 183 | 60 | . 7 1 | 1 | 170 | 66 | . 8 1 | 3 | 170 | 74 | . 9 1 | 3 | 185 | 57 | . 10 1 | 2 | 165 | 54 | . 11 2 | 1 | 170 | 50 | . 12 2 | 1 | 152 | 60 | . 13 2 | 3 | 173 | 63 | . 14 1 | 1 | 145 | 57 | . 15 1 | 3 | 163 | 77 | . 16 2 | 2 | 178 | 50 | . 17 2 | 2 | 163 | 57 | . 18 2 | 2 | 168 | 54 | . 19 2 | 3 | 170 | 57 | . height = df[&#39;height&#39;] weight = df[&#39;weight&#39;] . bmi5 = weight[4]/height[4]**2 bmi10 = weight[9]/height[9]**2 print(bmi5, bmi10) . 0.0030859375 0.0016654492330168006 . height1_5 = height[0:5] height1_5[0:3] . 0 183 1 168 2 160 Name: height, dtype: int64 . height1_5 = list(height[0:5]) height1_5[0:3] . [183, 168, 160] . # weight = [82, 52, 48, 50, 79, 73, 60, 66, 74, 57, 54, 50, 60, 63, 57, 77, 50, 57, 54, 57] . name_debt = &quot;나는 전북대학교 스페인중남미학과 김혜지입니다.&quot; name_debt[3:8] . &#39;전북대학교&#39; . 2-2 &#54032;&#45796;&#49828; &#45936;&#51060;&#53552; &#54532;&#47112;&#51076; . import pandas as pd . df = pd.DataFrame({ &#39;name&#39; : [&#39;이철수&#39;, &#39;김영희&#39;, &#39;홍길동&#39;, &#39;John Smith&#39;], &#39;sex&#39; : [&#39;M&#39;, &#39;F&#39;, &#39;M&#39;, &#39;M&#39;], &#39;age&#39; : [23, 25, 21, 33] }) . df . name sex age . 0 이철수 | M | 23 | . 1 김영희 | F | 25 | . 2 홍길동 | M | 21 | . 3 John Smith | M | 33 | . 슬라이싱 | . df[[&#39;name&#39;, &#39;age&#39;]] #두 개 이상의 열을 슬라이싱 하기위해서는 리스트를 사용하기 . name age . 0 이철수 | 23 | . 1 김영희 | 25 | . 2 홍길동 | 21 | . 3 John Smith | 33 | . #데이터 프레임 열을 슬라이스 하는 경우는 마침표 사용해서 슬라이싱이 가능 df.age . 0 23 1 25 2 21 3 33 Name: age, dtype: int64 . df[df[&#39;age&#39;] &gt;= 25] #df[&#39;age&#39;] &gt;= 25는 참, 거짓으로 구성된 시리즈 -&gt; 슬라이싱 하기 위해서는 df을 한 번 더 씌어주면 됨 . name sex age . 1 김영희 | F | 25 | . 3 John Smith | M | 33 | . 데이터 프레임 메소드 | . df.describe() #describe() : 숫자로 구성된 열의 기초 통계량을 구하는 표현식 . age . count 4.000000 | . mean 25.500000 | . std 5.259911 | . min 21.000000 | . 25% 22.500000 | . 50% 24.000000 | . 75% 27.000000 | . max 33.000000 | . df.max() #각 열의 최대값을 구하는 작업을 수행 ?왜 맥스 값을 찍었는데 홍길동 이름이 나오지? #한글은 가나다순, 영어는 알파벳 순으로 max값이 뽑힌다. . name 홍길동 sex M age 33 dtype: object . df.shape . (4, 3) . df.iloc[1,0] . &#39;김영희&#39; . df.iloc[2,2] . 21 . &#47928;&#51228;1 . 사진과 같은 데이터 프레임 만들기 | 데이터 프레임의 shape를 출력 | 평균 면적(area)을 계산하여라 | 인구수가 2000만명 이하인 주들을 골라내어라. 또한 해당하는 주의 평균 밀도를 구해보아라 | 첫 3개의 행과 첫 2개의 열로 이루어진 sub-dataframe을 추출하여라 | New York의 인구(pop)를 추출하여라 | df = pd.DataFrame({&quot;area&quot; : [423967, 695662, 141297, 170312, 149995], &quot;pop1&quot; : [38332521, 26448193, 19651127, 19552860, 12882135], &quot;density&quot; : [90.000000, 38.018740, 139.076746, 114.806121, 85.883763] }, index = [&quot;Califonia&quot;, &quot;Texas&quot;, &quot;New York&quot;, &quot;Flprida&quot;, &quot;Illinois&quot;]) df . area pop1 density . Califonia 423967 | 38332521 | 90.000000 | . Texas 695662 | 26448193 | 38.018740 | . New York 141297 | 19651127 | 139.076746 | . Flprida 170312 | 19552860 | 114.806121 | . Illinois 149995 | 12882135 | 85.883763 | . df.shape . (5, 3) . df[&quot;area&quot;].mean() #mean다음에 괄호 잊지 말기 () 중요!! . 316246.6 . df[df[&quot;pop1&quot;] &lt;= 20000000][&quot;density&quot;].mean() #mean() 다음에 괄호 필수, 데이터 프레임 옆에 붙여서 밀도의 평균 계산 가능 . 113.25554333333334 . df.iloc[:3,:2] # iloc 사용하기 . area pop1 . Califonia 423967 | 38332521 | . Texas 695662 | 26448193 | . New York 141297 | 19651127 | . df.iloc[2,1] . 19651127 . df[&quot;pop1&quot;][&quot;New York&quot;] # df를 뽑으려면 무조건 열 먼저 실행 후 행 선택하기(행 먼저하면 오류 발생) . 19651127 . 2-3 &#44256;&#49549;&#50676;&#52264; &#50668;&#44061; &#49688;&#49569;&#51088;&#47308; . import pandas as pd import matplotlib.pyplot as plt from matplotlib import rc %matplotlib inline rc(&#39;font&#39;, family=&#39;AppleGothic&#39;) plt.rcParams[&#39;axes.unicode_minus&#39;] = False plt.rcParams[&quot;figure.figsize&quot;] = (10,5) . url =&quot;https://ilovedata.github.io/teaching/bigdata2/data/train-data-01.csv&quot; train_raw_data = pd.read_csv(url) train_raw_data . TRAIN_NO DATE STATION_DEPART STATION_ARRV NUM_PASSENGER . 0 2 | 20190701 | 서울 | 대전 | 106.0 | . 1 2 | 20190702 | 서울 | 대전 | 113.0 | . 2 2 | 20190703 | 서울 | 대전 | 146.0 | . 3 2 | 20190704 | 서울 | 대전 | 84.0 | . 4 2 | 20190705 | 서울 | 대전 | 105.0 | . ... ... | ... | ... | ... | ... | . 1764 6 | 20190726 | 울산 | 부산 | 10.0 | . 1765 6 | 20190727 | 울산 | 부산 | 6.0 | . 1766 6 | 20190728 | 울산 | 부산 | 21.0 | . 1767 6 | 20190729 | 울산 | 부산 | 12.0 | . 1768 6 | 20190730 | 울산 | 부산 | 11.0 | . 1769 rows × 5 columns . num_train = train_raw_data[&quot;TRAIN_NO&quot;].unique() num_train . array([2, 5, 6]) . 다음 3개의 조건을 모두 만족하는 행들을 추출 . 열차번호가 2번이다. . 출발역은 서울역이다. . 도착역은 부산역이다. . a = train_raw_data[&quot;TRAIN_NO&quot;] == 2 b = train_raw_data[&quot;STATION_DEPART&quot;] == &quot;서울&quot; c = train_raw_data[&quot;STATION_ARRV&quot;] == &quot;부산&quot; d = a &amp; b &amp; c d . 0 False 1 False 2 False 3 False 4 False ... 1764 False 1765 False 1766 False 1767 False 1768 False Length: 1769, dtype: bool . train2 = train_raw_data[d] #습관처럼 df에 넣지 말자 정의된 데이터 프레임에 넣어야 한다. df[a] 이렇게 표현할 수 있는 이유는 df가 정의되어 있는 데이터 프레임이기 때문이다. . train2 . TRAIN_NO DATE STATION_DEPART STATION_ARRV NUM_PASSENGER . 60 2 | 20190701 | 서울 | 부산 | 613.0 | . 61 2 | 20190702 | 서울 | 부산 | 546.0 | . 62 2 | 20190703 | 서울 | 부산 | 492.0 | . 63 2 | 20190704 | 서울 | 부산 | 615.0 | . 64 2 | 20190705 | 서울 | 부산 | 572.0 | . 65 2 | 20190706 | 서울 | 부산 | 598.0 | . 66 2 | 20190707 | 서울 | 부산 | 326.0 | . 67 2 | 20190708 | 서울 | 부산 | 552.0 | . 68 2 | 20190709 | 서울 | 부산 | 540.0 | . 69 2 | 20190710 | 서울 | 부산 | 499.0 | . 70 2 | 20190711 | 서울 | 부산 | 562.0 | . 71 2 | 20190712 | 서울 | 부산 | 601.0 | . 72 2 | 20190713 | 서울 | 부산 | 655.0 | . 73 2 | 20190714 | 서울 | 부산 | 371.0 | . 74 2 | 20190715 | 서울 | 부산 | 557.0 | . 75 2 | 20190716 | 서울 | 부산 | 525.0 | . 76 2 | 20190717 | 서울 | 부산 | 557.0 | . 77 2 | 20190718 | 서울 | 부산 | 585.0 | . 78 2 | 20190719 | 서울 | 부산 | 593.0 | . 79 2 | 20190720 | 서울 | 부산 | 620.0 | . 80 2 | 20190721 | 서울 | 부산 | 355.0 | . 81 2 | 20190722 | 서울 | 부산 | 548.0 | . 82 2 | 20190723 | 서울 | 부산 | 557.0 | . 83 2 | 20190724 | 서울 | 부산 | 550.0 | . 84 2 | 20190725 | 서울 | 부산 | 593.0 | . 85 2 | 20190726 | 서울 | 부산 | 551.0 | . 86 2 | 20190727 | 서울 | 부산 | 615.0 | . 87 2 | 20190728 | 서울 | 부산 | 530.0 | . 88 2 | 20190729 | 서울 | 부산 | 589.0 | . 89 2 | 20190730 | 서울 | 부산 | 503.0 | . train2.plot(x=&#39;DATE&#39;, y=&quot;NUM_PASSENGER&quot;) #날짜별 승객데이터 확인 가능 . &lt;AxesSubplot:xlabel=&#39;DATE&#39;&gt; . to_datetime() :날짜 형식 변환 ex)format=&#39;%Y%m%d&#39; . train2[&#39;DATE2&#39;] = pd.to_datetime(train2[&#39;DATE&#39;], format=&#39;%Y%m%d&#39;) train2 . /var/folders/xh/xtwkcbrj0_l1srsb4r5pc3l00000gn/T/ipykernel_31932/279216404.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train2[&#39;DATE2&#39;] = pd.to_datetime(train2[&#39;DATE&#39;], format=&#39;%Y%m%d&#39;) . TRAIN_NO DATE STATION_DEPART STATION_ARRV NUM_PASSENGER DATE2 . 60 2 | 20190701 | 서울 | 부산 | 613.0 | 2019-07-01 | . 61 2 | 20190702 | 서울 | 부산 | 546.0 | 2019-07-02 | . 62 2 | 20190703 | 서울 | 부산 | 492.0 | 2019-07-03 | . 63 2 | 20190704 | 서울 | 부산 | 615.0 | 2019-07-04 | . 64 2 | 20190705 | 서울 | 부산 | 572.0 | 2019-07-05 | . 65 2 | 20190706 | 서울 | 부산 | 598.0 | 2019-07-06 | . 66 2 | 20190707 | 서울 | 부산 | 326.0 | 2019-07-07 | . 67 2 | 20190708 | 서울 | 부산 | 552.0 | 2019-07-08 | . 68 2 | 20190709 | 서울 | 부산 | 540.0 | 2019-07-09 | . 69 2 | 20190710 | 서울 | 부산 | 499.0 | 2019-07-10 | . 70 2 | 20190711 | 서울 | 부산 | 562.0 | 2019-07-11 | . 71 2 | 20190712 | 서울 | 부산 | 601.0 | 2019-07-12 | . 72 2 | 20190713 | 서울 | 부산 | 655.0 | 2019-07-13 | . 73 2 | 20190714 | 서울 | 부산 | 371.0 | 2019-07-14 | . 74 2 | 20190715 | 서울 | 부산 | 557.0 | 2019-07-15 | . 75 2 | 20190716 | 서울 | 부산 | 525.0 | 2019-07-16 | . 76 2 | 20190717 | 서울 | 부산 | 557.0 | 2019-07-17 | . 77 2 | 20190718 | 서울 | 부산 | 585.0 | 2019-07-18 | . 78 2 | 20190719 | 서울 | 부산 | 593.0 | 2019-07-19 | . 79 2 | 20190720 | 서울 | 부산 | 620.0 | 2019-07-20 | . 80 2 | 20190721 | 서울 | 부산 | 355.0 | 2019-07-21 | . 81 2 | 20190722 | 서울 | 부산 | 548.0 | 2019-07-22 | . 82 2 | 20190723 | 서울 | 부산 | 557.0 | 2019-07-23 | . 83 2 | 20190724 | 서울 | 부산 | 550.0 | 2019-07-24 | . 84 2 | 20190725 | 서울 | 부산 | 593.0 | 2019-07-25 | . 85 2 | 20190726 | 서울 | 부산 | 551.0 | 2019-07-26 | . 86 2 | 20190727 | 서울 | 부산 | 615.0 | 2019-07-27 | . 87 2 | 20190728 | 서울 | 부산 | 530.0 | 2019-07-28 | . 88 2 | 20190729 | 서울 | 부산 | 589.0 | 2019-07-29 | . 89 2 | 20190730 | 서울 | 부산 | 503.0 | 2019-07-30 | . train2.plot(x=&#39;DATE2&#39;, y=&quot;NUM_PASSENGER&quot;); . &#44536;&#47353;&#48324; &#48516;&#49437; . . train_raw_data[&#39;DATE2&#39;] = pd.to_datetime(train_raw_data[&#39;DATE&#39;], format=&#39;%Y%m%d&#39;) train_raw_data . TRAIN_NO DATE STATION_DEPART STATION_ARRV NUM_PASSENGER DATE2 . 0 2 | 20190701 | 서울 | 대전 | 106.0 | 2019-07-01 | . 1 2 | 20190702 | 서울 | 대전 | 113.0 | 2019-07-02 | . 2 2 | 20190703 | 서울 | 대전 | 146.0 | 2019-07-03 | . 3 2 | 20190704 | 서울 | 대전 | 84.0 | 2019-07-04 | . 4 2 | 20190705 | 서울 | 대전 | 105.0 | 2019-07-05 | . ... ... | ... | ... | ... | ... | ... | . 1764 6 | 20190726 | 울산 | 부산 | 10.0 | 2019-07-26 | . 1765 6 | 20190727 | 울산 | 부산 | 6.0 | 2019-07-27 | . 1766 6 | 20190728 | 울산 | 부산 | 21.0 | 2019-07-28 | . 1767 6 | 20190729 | 울산 | 부산 | 12.0 | 2019-07-29 | . 1768 6 | 20190730 | 울산 | 부산 | 11.0 | 2019-07-30 | . 1769 rows × 6 columns . 이제 날짜의 형식을 가진 변수 DATE2 를 아용하여 요일을 나타내는 새로운 변수 DAYOFWEEK 를 만들어 보자. .dt.dayofweek 를 적용하면 각 날짜에 대응하는 요일을 구해준다. . train_raw_data[&quot;DAYOFWEEK&quot;] = train_raw_data[&#39;DATE2&#39;].dt.dayofweek . 요일별, 구간별로 탑승객 수의 평균을 구하기 위하여 필요한 변수를 가진 데이터프레임을 먼저 만든다. . 요일별, 구간별 분석을 하려면 다음과 같은 3개의 변수(요일, 출발역, 도착역)로 자료를 그룹화(grouping)해야한다. . train_groub_data = train_raw_data[[&#39;STATION_DEPART&#39;, &#39;STATION_ARRV&#39;,&#39;DAYOFWEEK&#39;,&#39;NUM_PASSENGER&#39;]] . summary_data = train_raw_data.groupby([&#39;STATION_DEPART&#39;,&#39;STATION_ARRV&#39;,&#39;DAYOFWEEK&#39;]).mean() summary_data . TRAIN_NO DATE NUM_PASSENGER . STATION_DEPART STATION_ARRV DAYOFWEEK . 광명 대전 0 5.5 | 20190715.0 | 16.300 | . 1 5.5 | 20190716.0 | 15.200 | . 2 5.5 | 20190713.5 | 20.750 | . 3 5.5 | 20190714.5 | 21.125 | . 4 5.5 | 20190715.5 | 18.125 | . ... ... ... ... | ... | ... | . 행신 서울 2 2.0 | 20190713.5 | 12.750 | . 3 2.0 | 20190714.5 | 10.000 | . 4 2.0 | 20190715.5 | 8.250 | . 5 2.0 | 20190716.5 | 7.500 | . 6 2.0 | 20190717.5 | 5.000 | . 224 rows × 3 columns . 위의 그룹화 결과 자료는 다루기가 까다로와서 각 그룹에 대한 값을 열로 다시 만들어 주는 것이 좋다. . reset_index()사용, inplace=True 은 실제로 데이터프레임에 적용하라는 의미 . summary_data.reset_index(inplace=True) summary_data . STATION_DEPART STATION_ARRV DAYOFWEEK TRAIN_NO DATE NUM_PASSENGER . 0 광명 | 대전 | 0 | 5.5 | 20190715.0 | 16.300 | . 1 광명 | 대전 | 1 | 5.5 | 20190716.0 | 15.200 | . 2 광명 | 대전 | 2 | 5.5 | 20190713.5 | 20.750 | . 3 광명 | 대전 | 3 | 5.5 | 20190714.5 | 21.125 | . 4 광명 | 대전 | 4 | 5.5 | 20190715.5 | 18.125 | . ... ... | ... | ... | ... | ... | ... | . 219 행신 | 서울 | 2 | 2.0 | 20190713.5 | 12.750 | . 220 행신 | 서울 | 3 | 2.0 | 20190714.5 | 10.000 | . 221 행신 | 서울 | 4 | 2.0 | 20190715.5 | 8.250 | . 222 행신 | 서울 | 5 | 2.0 | 20190716.5 | 7.500 | . 223 행신 | 서울 | 6 | 2.0 | 20190717.5 | 5.000 | . 224 rows × 6 columns . 이제 원하는 출발역과 도착역을 선택하여 요일별 평균 탑승객 수를 살펴보자. . od_average = summary_data[ ( summary_data[&#39;STATION_DEPART&#39;] == &#39;서울&#39; ) &amp; ( summary_data[&#39;STATION_ARRV&#39;] == &#39;부산&#39; )] od_average . STATION_DEPART STATION_ARRV DAYOFWEEK TRAIN_NO DATE NUM_PASSENGER . 98 서울 | 부산 | 0 | 4.333333 | 20190715.0 | 284.800000 | . 99 서울 | 부산 | 1 | 4.333333 | 20190716.0 | 284.266667 | . 100 서울 | 부산 | 2 | 4.333333 | 20190713.5 | 287.833333 | . 101 서울 | 부산 | 3 | 4.333333 | 20190714.5 | 319.500000 | . 102 서울 | 부산 | 4 | 4.333333 | 20190715.5 | 353.833333 | . 103 서울 | 부산 | 5 | 4.333333 | 20190716.5 | 311.166667 | . 104 서울 | 부산 | 6 | 4.333333 | 20190717.5 | 265.083333 | . od_average.plot(x = &quot;DAYOFWEEK&quot;, y = &quot;NUM_PASSENGER&quot;); . 2-4 &#50612;&#47536;&#50773;&#51088; . import matplotlib.pyplot as plt from matplotlib import rc plt.rcParams[&#39;axes.unicode_minus&#39;] = False plt.rcParams[&quot;figure.figsize&quot;] = (10,5) import numpy as np import pandas as pd . import base64 import requests url_data =&quot;https://ilovedata.github.io/teaching/bigdata2/data/little-prince.txt&quot; little_prince = requests.get(url_data) little_prince = little_prince.text . little_prince[:1000] . &#39; r n 어린 왕자 r n 영어동화 (우리말 해석) r n 생텍쥐페리 r n r n r n 헌사(받치는 글) r n 레옹 베르트에게 r n r n 먼저 이 글을 어린이들이 아닌 어른들에게 바치는 것에 대해 사과할까 한다. 심심한 사과의 말을 전한다. 하지만 이들 어른들은 세상에서 가장 좋은 친구다. 물론 이 발언에 대해서도 심심한 사과의 말을 전하는 바이다. 이들 어른들은 모든 걸 이해할 수 있다, 심지어 어린이들의 책까지. 이런 세 번째로 사과의 말을 전하는 바이다. 이들 어른들은 프랑스에만 해도 굶 주림과 추위에 살고 있다. 그에겐 아늑함이 필요한 것도 사실이다. 내 사과들이 충분하든 아니든, 난 이 책을 어른이 된 어린이들에게 바치는 바이다. 모든 어른도 출발은 아이들이었다. 하지만 그들 몇몇만 그걸 기억해낼 뿐이다. 그래 내 헌사(받 치는 글)를 다음과 같이 수정할까 한다. r n 어린이였을 때의 r n 레옹 베르트에게 r n r n ※ 지금부터는 『어린 왕자』(생텍쥐페리의 동화)를 해석해보겠습니다. r n r n 이 동화는 아래 링크의 동화를 우리말로 옮긴 것입니다. r n (번역과정에서 구글 번역기를 이용해 1차로 영어원문으로 만든 다음, 그걸 우리말로 읽을 수 있게 좀 수정하고 다듬은 후, 2차로 우리말로 해석했습니다. 따라서 프랑스어 원문과 다를 수 있습니다. 하지만 성실히 번역했기에 큰 줄거리는 맞다고 생각됩니다. 완전한 해석은 아니라는 점 감안해주시고 읽어주세용~♥ 어린 왕자가 좋아서 해석해본 거예요, 제가 읽으려고요. 그러니 많이 읽어주세요~) r n r n 『어린 왕자』(호주의 애들레이드대학교의 인터넷도서관 사이트)(프랑스 원문) ▶ https://ebooks.adelaide.edu.au/s/saint- exupery/antoine_de/le-petit-prince/ (검색 일자 : 2017-12-9) r n r n r n r n Le Petit Prince r n Le Petit Prince / Antoine de &#39; . type(little_prince) . str . np.char.count(자료, 찾고자 하는 문자열) : 문자열 변수안에 지정된 문자열이 몇 번 나타나는지 세어주는 함수 . np.char.count(little_prince, &quot;별&quot;) . array(159) . &#53581;&#49828;&#53944; &#45208;&#45572;&#44592; . mychar = &quot;폭넓은 교양과 심오한 학문적 이론 및 창의적 전문기술을 지닌 지성인을 기른다. 성실한 근면을 바탕으로 책임과 의무를 다하는 건전한 인격을 갖춘 민주 시민을 기른다.&quot; mychar . &#39;폭넓은 교양과 심오한 학문적 이론 및 창의적 전문기술을 지닌 지성인을 기른다. 성실한 근면을 바탕으로 책임과 의무를 다하는 건전한 인격을 갖춘 민주 시민을 기른다.&#39; . mychar.split(&quot;을&quot;) . [&#39;폭넓은 교양과 심오한 학문적 이론 및 창의적 전문기술&#39;, &#39; 지닌 지성인&#39;, &#39; 기른다. 성실한 근면&#39;, &#39; 바탕으로 책임과 의무를 다하는 건전한 인격&#39;, &#39; 갖춘 민주 시민&#39;, &#39; 기른다.&#39;] . little_prince_chapters = little_prince.split(&#39;어린 왕자 r n&#39;) . little_prince_chapters . final = pd.DataFrame({&quot;chapters&quot; : little_prince_chapters}) final . chapters . 0 r n | . 1 영어동화 (우리말 해석) r n 생텍쥐페리 r n r n r n... | . 2 r n 물론 내 그림은 실제 모습보단 덜해. 그렇다고 내 실수는 아니라고... | . 3 4장 r n r n 난 곧 아주 중요한 두 번째 사실도 알게 됐는데,... | . 4 5장 r n r n 난 매일 그 별과 떠나온 이유와 여행에 대해 알게... | . 5 6장 r n r n 아! 어린 왕자여, 난 이제야 알겠어, 조금씩, ... | . 6 7장 r n r n 다섯 째 날에도, 항상 양 덕분에, 어린 왕자의 ... | . 7 8장 r n r n 나는 곧 이 꽃에 대해 알게 되었다. 어린 왕자의... | . 8 9장 r n 내 생각에, 어린 왕자는, 철새들이 이동할 때 함께 그 별 을... | . 9 10장 r n 어린 왕자의 별 가까이에 소행성 325호, 326호, 327... | . 10 11장 r n r n 두 번째 별엔 허영심쟁이가 살고 있었어요. r ... | . 11 12장 r n r n 술꾼 r n 다음으로 간 별엔 술꾼이 살고 ... | . 12 13장 r n r n 네 번째 별엔 장사꾼이 살고 있었다. 어린 왕자... | . 13 14장 r n r n 다섯 번째 별은 좀 이상했다. 가장 작았기 때문... | . 14 15장 r n r n 여섯 번째 별은 열 배는 큰 별이었다. 거기엔 ... | . 15 16장 r n r n 그리하여 일곱 번째로 들른 별은 지구였다. r ... | . 16 17장 r n r n 뭔 말을 거창하게 하려다 보면, 허풍이 좀 들어... | . 17 18장 r n r n 어린 왕자는 사막을 거닐어보았지만 마주친 거라 ... | . 18 19장 r n r n 어린 왕자는 높은 산에 올랐다. 어린 왕자의 별... | . 19 20장 r n r n 하지만 어린 왕자는 모랫길과 바위와 눈 뿐인 곳을 한... | . 20 21장 r n r n 그런데 그때 여우가 나타났다. r n &quot;안녕... | . 21 &quot;넌 누구니?&quot;라며 어린 왕자가 말했다. &quot;근데 넌 참 귀엽구나...&quot; r ... | . 22 22장 r n r n &quot;안녕하세요.&quot;라며 어린 왕자가 말했다. &quot;안녕... | . 23 23장 r n r n &quot;안녕하세요.&quot;라며 어린 왕자가 말했다. &quot;안녕... | . 24 24장 r n r n 비행기 고장으로 사막에 떨어진지도 이제 여덟째 ... | . 25 25장 r n r n &quot;사람들은,&quot;라며 어린 왕자가 말했다. &quot;서둘러... | . 26 26장 r n r n 그 우물 가 옆엔 무너진 돌담 하나가 있었다. ... | . 27 27장 r n r n 물론 지금은 6년이 지난 얘기다... 난 아직 ... | . &#47928;&#51088;&#50676; &#48712;&#46020; &#44228;&#49328; . counts = final.applymap(lambda x: np.char.count(x, &quot;어린 왕자&quot;)) counts . chapters . 0 0 | . 1 5 | . 2 6 | . 3 7 | . 4 10 | . 5 2 | . 6 5 | . 7 14 | . 8 8 | . 9 24 | . 10 10 | . 11 6 | . 12 11 | . 13 12 | . 14 14 | . 15 0 | . 16 13 | . 17 4 | . 18 7 | . 19 8 | . 20 1 | . 21 23 | . 22 6 | . 23 3 | . 24 12 | . 25 15 | . 26 20 | . 27 12 | . counts.plot(); . &#49892;&#49845; 1. &#52309;&#53552;&#48324;&#47196; &#50668;&#50864;, ?, !&#51032; &#48712;&#46020;&#47484; &#44228;&#49328;&#54616;&#44256; &#44536;&#47548;&#51004;&#47196; &#45208;&#53440;&#45236;&#50612;&#46972;. . counts_fox = final.applymap(lambda x : np.char.count(x, &quot;여우&quot;)) counts_fox . chapters . 0 0 | . 1 0 | . 2 0 | . 3 0 | . 4 0 | . 5 0 | . 6 0 | . 7 0 | . 8 0 | . 9 0 | . 10 0 | . 11 0 | . 12 0 | . 13 0 | . 14 0 | . 15 0 | . 16 0 | . 17 0 | . 18 0 | . 19 0 | . 20 3 | . 21 30 | . 22 0 | . 23 0 | . 24 4 | . 25 2 | . 26 0 | . 27 0 | . counts_fox.plot() . &lt;AxesSubplot:&gt; . counts_n = final.applymap(lambda x: np.char.count(x, &quot;!&quot;)) counts_n . chapters . 0 0 | . 1 2 | . 2 11 | . 3 4 | . 4 7 | . 5 3 | . 6 19 | . 7 9 | . 8 1 | . 9 18 | . 10 5 | . 11 1 | . 12 9 | . 13 6 | . 14 14 | . 15 1 | . 16 4 | . 17 0 | . 18 1 | . 19 2 | . 20 0 | . 21 16 | . 22 0 | . 23 0 | . 24 3 | . 25 8 | . 26 17 | . 27 8 | . counts_n.plot() . &lt;AxesSubplot:&gt; . counts_m = final.applymap(lambda x: np.char.count(x, &quot;?&quot;)) counts_m . chapters . 0 0 | . 1 2 | . 2 10 | . 3 7 | . 4 4 | . 5 2 | . 6 7 | . 7 3 | . 8 1 | . 9 11 | . 10 5 | . 11 4 | . 12 20 | . 13 6 | . 14 16 | . 15 0 | . 16 5 | . 17 2 | . 18 5 | . 19 1 | . 20 0 | . 21 13 | . 22 5 | . 23 2 | . 24 2 | . 25 5 | . 26 7 | . 27 2 | . counts_m.plot() . &lt;AxesSubplot:&gt; .",
            "url": "https://g-gg-ggg.github.io/Oops/python/2022/09/29/2%EA%B0%95%EC%A0%95%EB%A6%AC.html",
            "relUrl": "/python/2022/09/29/2%EA%B0%95%EC%A0%95%EB%A6%AC.html",
            "date": " • Sep 29, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://g-gg-ggg.github.io/Oops/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://g-gg-ggg.github.io/Oops/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://g-gg-ggg.github.io/Oops/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://g-gg-ggg.github.io/Oops/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}